{
    "test_llama2_legacy_mmlu_evaluate": [
        {
            "param": {
                "task-data-path":"/data/eval_data/mmlu/data/test",
                "task":"mmlu",
                "tensor-model-parallel-size": 8,
                "pipeline-model-parallel-size": 1, 
                "num-layers": 32,
                "hidden-size": 4096,
                "ffn-hidden-size": 11008,
                "num-attention-heads": 32, 
                "seq-length": 4096,
                "max-new-tokens": 1,
                "max-position-embeddings": 4096,
                "make-vocab-size-divisible-by": 1, 
                "micro-batch-size": 1, 
                "normalization": "RMSNorm", 
                "position-embedding-type": "rope",
                "swiglu": null,
                "tokenizer-not-use-fast": null,
                "untie-embeddings-and-output-weights": null,
                "disable-bias-linear": null,
                "fp16": null,
                "no-load-rng": null,
                "no-load-optim": null,
                "load":"/data/pipeline/llama2-7B-tp8-pp1",
                "tokenizer-type":"PretrainedFromHF",  
                "tokenizer-name-or-path":"/data/hf/llama-2-7b-hf",
                "use-deter-comp": null
            }
        }
    ],

    "test_qwen_legacy_prompt_mmlu_evaluate": [
        {
            "param": {
                "task-data-path":"/data/eval_data/full/mmlu/test",
                "task":"mmlu",
                "tensor-model-parallel-size": 8,
                "pipeline-model-parallel-size": 1, 
                "num-layers": 32,
                "hidden-size": 4096,
                "ffn-hidden-size": 11008,
                "num-attention-heads": 32, 
                "seq-length": 8192,
                "max-new-tokens": 1,
                "max-position-embeddings": 32768,
                "make-vocab-size-divisible-by": 16, 
                "micro-batch-size": 2, 
                "normalization": "RMSNorm", 
                "position-embedding-type": "rope",
                "swiglu": null,
                "tokenizer-not-use-fast": null,
                "untie-embeddings-and-output-weights": null,
                "disable-bias-linear": null,
                "bf16": null,
                "no-load-rng": null,
                "no-load-optim": null,
                "load":"/data/pipeline/Qwen-7B-tp8-pp1/",
                "tokenizer-type":"PretrainedFromHF",  
                "tokenizer-name-or-path":"/data/qwen-7b/",
                "use-deter-comp": null,
                "exit-on-missing-checkpoint": null,
                "max-eval-samples": 2,
                "prompt-type": "qwen",
                "add-qkv-bias":null
            }
        }
    ], 
    
    "test_qwen_legacy_prompt_boolq_evaluate": [
        {
            "param": {
                "task-data-path":"/data/eval_data/full/boolq/test",
                "task":"boolq",
                "tensor-model-parallel-size": 8,
                "pipeline-model-parallel-size": 1, 
                "num-layers": 32,
                "hidden-size": 4096,
                "ffn-hidden-size": 11008,
                "num-attention-heads": 32, 
                "seq-length": 8192,
                "max-new-tokens": 1,
                "max-position-embeddings": 32768,
                "make-vocab-size-divisible-by": 16, 
                "micro-batch-size": 2, 
                "normalization": "RMSNorm", 
                "position-embedding-type": "rope",
                "swiglu": null,
                "tokenizer-not-use-fast": null,
                "untie-embeddings-and-output-weights": null,
                "disable-bias-linear": null,
                "bf16": null,
                "no-load-rng": null,
                "no-load-optim": null,
                "load":"/data/pipeline/Qwen-7B-tp8-pp1/",
                "tokenizer-type":"PretrainedFromHF",  
                "tokenizer-name-or-path":"/data/qwen-7b/",
                "use-deter-comp": null,
                "exit-on-missing-checkpoint": null,
                "max-eval-samples": 30,
                "prompt-type": "qwen",
                "add-qkv-bias":null
            }
        }
    ],

    "test_qwen_legacy_prompt_ceval_evaluate": [
        {
            "param": {
                "task-data-path":"/data/eval_data/full/ceval/val",
                "task":"ceval",
                "tensor-model-parallel-size": 8,
                "pipeline-model-parallel-size": 1, 
                "num-layers": 32,
                "hidden-size": 4096,
                "ffn-hidden-size": 11008,
                "num-attention-heads": 32, 
                "seq-length": 8192,
                "max-new-tokens": 1,
                "max-position-embeddings": 32768,
                "make-vocab-size-divisible-by": 16, 
                "micro-batch-size": 2, 
                "normalization": "RMSNorm", 
                "position-embedding-type": "rope",
                "swiglu": null,
                "tokenizer-not-use-fast": null,
                "untie-embeddings-and-output-weights": null,
                "disable-bias-linear": null,
                "bf16": null,
                "no-load-rng": null,
                "no-load-optim": null,
                "load":"/data/pipeline/Qwen-7B-tp8-pp1/",
                "tokenizer-type":"PretrainedFromHF",  
                "tokenizer-name-or-path":"/data/qwen-7b/",
                "use-deter-comp": null,
                "exit-on-missing-checkpoint": null,
                "max-eval-samples": 2,
                "prompt-type": "qwen",
                "add-qkv-bias":null
            }
        }
    ],

    "test_llama2_lora_legacy_mmlu_evaluate": [
        {
            "param": {
                "task-data-path":"/data/eval_data/mmlu/data/test",
                "task":"mmlu",
                "tensor-model-parallel-size": 8,
                "pipeline-model-parallel-size": 1, 
                "num-layers": 32,
                "hidden-size": 4096,
                "ffn-hidden-size": 11008,
                "num-attention-heads": 32, 
                "seq-length": 4096,
                "max-new-tokens": 1,
                "max-position-embeddings": 4096,
                "make-vocab-size-divisible-by": 1, 
                "micro-batch-size": 1, 
                "normalization": "RMSNorm", 
                "position-embedding-type": "rope",
                "swiglu": null,
                "tokenizer-not-use-fast": null,
                "untie-embeddings-and-output-weights": null,
                "disable-bias-linear": null,
                "fp16": null,
                "no-load-rng": null,
                "no-load-optim": null,
                "load":"/data/pipeline/llama2-7B-tp8-pp1",
                "tokenizer-type":"PretrainedFromHF",  
                "tokenizer-name-or-path":"/data/hf/llama-2-7b-hf",
                "use-deter-comp": null,
                "use-fused-rmsnorm": null,
                "exit-on-missing-checkpoint": null,
                "lora-load": "/data/llama-2-7b-lora-tp8-pp1",
                "lora-r": 16,
                "lora-alpha": 32,
                "lora-target-modules": ["query_key_value", "dense", "dense_h_to_4h","dense_4h_to_h"],
                "seed": 42
            }
        }
    ],

    "test_llama2_mcore_humaneval_evaluate": [
        {
            "param": {
                "task-data-path":"/data/eval_data/human_eval",
                "task":"human_eval",
                "tensor-model-parallel-size": 8,
                "pipeline-model-parallel-size": 1,
                "num-layers": 32,
                "use-mcore-models": null,
                "hidden-size": 4096,
                "ffn-hidden-size": 11008,
                "num-attention-heads": 32,
                "seq-length": 4096,
                "max-new-tokens": 200,
                "evaluation-batch-size": 1,
                "max-position-embeddings": 4096,
                "make-vocab-size-divisible-by": 1,
                "micro-batch-size": 1,
                "normalization": "RMSNorm",
                "position-embedding-type": "rope",
                "swiglu": null,
                "tokenizer-not-use-fast": null,
                "untie-embeddings-and-output-weights": null,
                "disable-bias-linear": null,
                "no-masked-softmax-fusion": null,
                "fp16": null,
                "no-load-rng": null,
                "no-load-optim": null,
                "load":"/data/ci/ckpt",
                "tokenizer-type":"PretrainedFromHF",
                "tokenizer-name-or-path":"/data/hf/llama-2-7b-hf",
                "use-deter-comp": null
            }
        }
    ]
}
