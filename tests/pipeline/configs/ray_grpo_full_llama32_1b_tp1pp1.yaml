defaults:
  - model:
      - llama32-1b

training:
  global_batch_size: 1
  seq_length: 309
  tokenizer_type: PretrainedFromHF
  tokenizer_name_or_path: /data/ppo/llama-3.2-1b-instruct/
  train_iters: 15
  distributed_backend: nccl
  no_shared_storage: true
  save_interval: 10000
  no_load_optim: true
  no_load_rng: true
  bf16: true
  is_instruction_dataset: true
  variable_seq_lengths: true
  no_shuffle: true
  stage: ray_grpo
  sequence_parallel: False
  use_deter_comp: True

actor_rollout_ref:
  actor_rollout:
    model: llama32-1b
    do_sample: true
    micro_batch_size: 1
    ppo_mini_batch_size: 1
    num_samples_per_step: 1
    max_prompt_length: 256
    ppo_epochs: 1
    clip_ratio: 0.2
    entropy_coeff: 0.001
    shuffle_minibatch: false
    use_kv_cache: true
    tensor_model_parallel_size: 1
    pipeline_model_parallel_size: 1
    lr: 1e-7
    lr_decay_style: constant
    min_lr: 0.0
    weight_decay: 0.0
    lr_warmup_fraction: 0.0
    clip_grad: 10000.0
    adam_beta1: 0.9
    adam_beta2: 0.999
    initial_loss_scale: 4096
    finetune: true
    load: /data/ppo/llama-3.2-1b-instruct-tp1-pp1
    save: ./ckpt
    num_gpus_for_train: 1
    num_gpus_for_infer: 1
    pad_to_multiple_of: 1
    data_path: /data/ppo/llama32-ppo-trl/alpaca
    split: 100,0,0
    n_samples_per_prompt: 2
  ref:
    model: llama32-1b
    tensor_model_parallel_size: 1
    pipeline_model_parallel_size: 1
    micro_batch_size: 1
    load: /data/ppo/llama-3.2-1b-instruct-tp1-pp1

reward:
  model: llama32-1b
  tensor_model_parallel_size: 1
  pipeline_model_parallel_size: 1
  micro_batch_size: 1
  load: /data/ppo/llama-3.2-1b-rm-mcore-tp1-pp1

algorithm:
  gamma: 1.0
  lam: 0.95
  adv_estimator: group_norm
  kl_penalty: kl
  kl_ctrl:
    type: fixed
    kl_coef: 0.05
  missing_eos_penalty: 0.0

resource_pool:
  actor_rollout: [2]
  ref: [1]
  reward: [1]